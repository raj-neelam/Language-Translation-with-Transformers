{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112fb076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "# writer = SummaryWriter('../../runs/simple_transformer_16_4L_128B')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8a45bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109128</th>\n",
       "      <td>So here's my question to you:</td>\n",
       "      <td>तो मेरा सवाल आप लोगों से यह है कि:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26212</th>\n",
       "      <td>Khan was more blunt 24 hours later . “ If the ...</td>\n",
       "      <td>24 घंटे बाद खान के सुर तीखे थे , ' ' स्थिति ऐस...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81823</th>\n",
       "      <td>The Kotwal nearly combined all the functions o...</td>\n",
       "      <td>कोतवाल में न्यायिक शक्तियों के अतिरिक्त मुहतसि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>5 . Inject this air into the bottle of insulin...</td>\n",
       "      <td>अब इस हवा को इंसुलिन की वायल में इंजेक्ट करें....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41876</th>\n",
       "      <td>You should also ask for a written description ...</td>\n",
       "      <td>आपको सामान का लिखित वर्णन भी माँगना चाहिए .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  English  \\\n",
       "109128                      So here's my question to you:   \n",
       "26212   Khan was more blunt 24 hours later . “ If the ...   \n",
       "81823   The Kotwal nearly combined all the functions o...   \n",
       "4072    5 . Inject this air into the bottle of insulin...   \n",
       "41876   You should also ask for a written description ...   \n",
       "\n",
       "                                                    Hindi  \n",
       "109128                 तो मेरा सवाल आप लोगों से यह है कि:  \n",
       "26212   24 घंटे बाद खान के सुर तीखे थे , ' ' स्थिति ऐस...  \n",
       "81823   कोतवाल में न्यायिक शक्तियों के अतिरिक्त मुहतसि...  \n",
       "4072    अब इस हवा को इंसुलिन की वायल में इंजेक्ट करें....  \n",
       "41876         आपको सामान का लिखित वर्णन भी माँगना चाहिए .  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../data/Dataset_English_Hindi.csv\")\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac125bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data['English'].apply(lambda x: isinstance(x, str)) & data['Hindi'].apply(lambda x: isinstance(x, str))\n",
    "data = data.loc[mask].copy()\n",
    "data['English'] = data['English'].str.lower()\n",
    "data['Hindi'] = data['Hindi'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e9d7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filtering: 81178\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = 16\n",
    "data = data[data['English'].str.split().apply(len) < MAX_SENT_LEN].copy()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(f\"Rows after filtering: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a566f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Distribution of English Sentence Lengths'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['English'].str.split().apply(len).plot(kind='hist', bins=15, title='Distribution of English Sentence Lengths', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f05dc2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, texts):\n",
    "        self.oov_token = \"<|unknown|>\"\n",
    "        self.start_token = \"<|startoftext|>\"\n",
    "        self.end_token = \"<|endoftext|>\"\n",
    "        self.padding_token = \"<|pad|>\"\n",
    "        self.word_index = {self.oov_token: 0, self.start_token: 1, self.end_token: 2, self.padding_token: 3}\n",
    "    \n",
    "        for text in texts:\n",
    "            text = text.replace('!', '')\n",
    "            text = text.replace('.', '')\n",
    "            text = text.replace('(', '')\n",
    "            text = text.replace(')', '')\n",
    "            text = text.replace('?', '')\n",
    "            text = text.replace('-', ' ')\n",
    "\n",
    "            text = text.split()\n",
    "            for word in text:\n",
    "                if word not in self.word_index:\n",
    "                    self.word_index[word] = len(self.word_index)\n",
    "        self.index_word = {idx : word for word, idx in self.word_index.items()}\n",
    "\n",
    "        self.vocab_size = len(self.word_index)\n",
    "    def encode(self, texts):\n",
    "        tokenized_texts = []\n",
    "        for text in texts:\n",
    "            text = text.replace('!', '')\n",
    "            text = text.replace('.', '')\n",
    "            text = text.replace('(', '')\n",
    "            text = text.replace(')', '')\n",
    "            text = text.replace('?', '')\n",
    "            text = text.replace('-', ' ')\n",
    "            tokenized_text = []\n",
    "            text = text.split()\n",
    "            for word in text:\n",
    "                tokenized_text.append(self.word_index.get(word, self.word_index[self.oov_token]))\n",
    "            tokenized_texts.append(tokenized_text)\n",
    "        return tokenized_texts\n",
    "    def decode(self, sequences):\n",
    "        decoded_texts = []\n",
    "        for sequence in sequences:\n",
    "            decoded_text = []\n",
    "            for index in sequence:\n",
    "                decoded_text.append(self.index_word.get(index, self.oov_token))\n",
    "            decoded_texts.append(' '.join(decoded_text))\n",
    "        return decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643d75f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 45241\n",
      "Hindi Vocabulary Size: 45022\n"
     ]
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(data['English'])\n",
    "hin_tokenizer = Tokenizer(data['Hindi'])\n",
    "print(f\"English Vocabulary Size: {eng_tokenizer.vocab_size}\")# 80223\n",
    "print(f\"Hindi Vocabulary Size: {hin_tokenizer.vocab_size}\")# 85059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f4ecc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 81, 82, 61, 8589, 28347, 0, 0], [93, 103, 109, 2143], [122, 103, 182, 305]]\n",
      "['hello how are you raj neelam <|unknown|> <|unknown|>', 'this is a test', 'what is your name']\n",
      "\n",
      "[[28587, 121, 122, 123, 3341, 30020, 5351, 0], [142, 110, 15076, 79], [403, 432, 113, 79]]\n",
      "['नमस्ते आप कैसे हैं राज नीलम गौरव <|unknown|>', 'यह एक परीक्षण है', 'तुम्हारा नाम क्या है']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = eng_tokenizer.encode([\"hello how are you raj neelam gaurav convolution\", \"this is a test\", \"what is your name\"])\n",
    "print(tokenized_text)\n",
    "print(eng_tokenizer.decode(tokenized_text))\n",
    "print()\n",
    "tokenized_text = hin_tokenizer.encode([\"नमस्ते आप कैसे हैं राज नीलम गौरव कन्वोल्यूशन\", \"यह एक परीक्षण है\", \"तुम्हारा नाम क्या है\"])\n",
    "print(tokenized_text)\n",
    "print(hin_tokenizer.decode(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2284e149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73060, 8118)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data, test_data = train_test_split(data, test_size=0.1)\n",
    "len(data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1df5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 128\n",
    "NEM_LAYER = 4\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "START_LR = 0.001\n",
    "END_LR = 0.0001\n",
    "TOTAL_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64bf46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, eng_tokenizer, hin_tokenizer):\n",
    "        self.data = data\n",
    "        self.eng_tokenizer = eng_tokenizer\n",
    "        self.hin_tokenizer = hin_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng_text = self.data.iloc[idx]['English']\n",
    "        hin_text = self.data.iloc[idx]['Hindi']\n",
    "\n",
    "        eng_tokenized = self.eng_tokenizer.encode([eng_text])[0]\n",
    "        if len(eng_tokenized) > MAX_SENT_LEN:\n",
    "            eng_tokenized = eng_tokenized[:MAX_SENT_LEN]\n",
    "\n",
    "        eng_padded = [self.eng_tokenizer.word_index[self.eng_tokenizer.padding_token]] * (MAX_SENT_LEN - len(eng_tokenized)) + eng_tokenized\n",
    "\n",
    "        hin_tokenized = self.hin_tokenizer.encode([hin_text])[0]\n",
    "\n",
    "        if len(hin_tokenized) > MAX_SENT_LEN - 2:\n",
    "            hin_tokenized = hin_tokenized[:MAX_SENT_LEN - 2]\n",
    "        hin_padded = [self.hin_tokenizer.word_index[self.hin_tokenizer.start_token]] + hin_tokenized + [self.hin_tokenizer.word_index[self.hin_tokenizer.end_token]] + [self.hin_tokenizer.word_index[self.hin_tokenizer.padding_token]] * (MAX_SENT_LEN - len(hin_tokenized) - 2)\n",
    "\n",
    "        return {\n",
    "            'eng_input': torch.tensor(eng_padded),\n",
    "            'hin_target': torch.tensor(hin_padded)\n",
    "        }\n",
    "\n",
    "dataset = DataLoader(MyDataset(data, eng_tokenizer, hin_tokenizer), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataset = DataLoader(MyDataset(test_data, eng_tokenizer, hin_tokenizer), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c300d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_encoding(context, dimension):\n",
    "    pos = np.arange(context)[:, np.newaxis]\n",
    "    div_term = 1 / (10000 ** (np.arange(0, dimension, 2) / dimension))\n",
    "    \n",
    "    enc = np.zeros((context, dimension))\n",
    "    enc[:, 0::2] = np.sin(pos * div_term)\n",
    "    enc[:, 1::2] = np.cos(pos * div_term)\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29a26b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Q = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "        self.K = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "        self.V = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "        scores = attention_scores / (EMBED_DIM ** 0.5)\n",
    "        scores = scores.softmax(dim=-1)\n",
    "\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a8cae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masked_Self_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Q = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "        self.K = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "        self.V = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "\n",
    "        attention_scores = q @ k.transpose(-2, -1)\n",
    "        scores = attention_scores / (EMBED_DIM ** 0.5)\n",
    "\n",
    "        B, C, _ = x.shape\n",
    "        mask = torch.tril(torch.ones(C, C, device=x.device)).bool()\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        scores = scores.softmax(dim=-1)\n",
    "\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc60c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "        self.K = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "        self.V = nn.Linear(EMBED_DIM, EMBED_DIM)\n",
    "    def forward(self, hin_vec, eng_vec):\n",
    "        Q = self.Q(hin_vec)\n",
    "        K = self.K(eng_vec)\n",
    "        V = self.V(eng_vec)\n",
    "        \n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (EMBED_DIM ** 0.5)\n",
    "        scores = attention_scores.softmax(dim=-1)\n",
    "\n",
    "        output = torch.matmul(scores, V)\n",
    "        return output    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78518f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Encoder_Block(nn.Module):\n",
    "    def __init__(self): # inp_dim is vocab size of the english tokenizer\n",
    "        super().__init__()\n",
    "        self.self_attention = Self_Attention()\n",
    "        self.norm1 = nn.LayerNorm(EMBED_DIM)\n",
    "        self.norm2 = nn.LayerNorm(EMBED_DIM)\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(EMBED_DIM, 3*EMBED_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3*EMBED_DIM, EMBED_DIM)\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(DROPOUT_RATE)\n",
    "        self.dropout2 = nn.Dropout(DROPOUT_RATE)\n",
    "\n",
    "\n",
    "    def forward(self, words_vec):  \n",
    "        informed_words = self.self_attention(words_vec)   # (B, C, E) Attention\n",
    "        informed_words = self.dropout1(informed_words) # droupout\n",
    "\n",
    "        words_vec = words_vec + informed_words\n",
    "        words_vec = self.norm1(words_vec) # (B, C, E) Add & Normalize\n",
    "\n",
    "        thought_words = self.ffnn(words_vec) # feef forward neural network\n",
    "\n",
    "        thought_words = self.dropout2(thought_words) # droupout\n",
    "        words_vec = words_vec + thought_words\n",
    "        words_vec = self.norm2(words_vec) # (B, C, E) Add & Normalize\n",
    "\n",
    "        return words_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3630f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(input_dim, EMBED_DIM)\n",
    "        pe = torch.tensor(\n",
    "            create_positional_encoding(MAX_SENT_LEN, EMBED_DIM),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.register_buffer(\"positional_encoding\", pe) # register and save positional encodings\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Transformer_Encoder_Block() for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):                             # (B, C)\n",
    "        embedded = self.word_embedding(x)               # (B, C, E) batch, context_length, embedding\n",
    "        words_vec = self.positional_encoding[:embedded.size(1)] + embedded   # (B, C, E) Adding positonal embeddings\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            words_vec = block(words_vec)\n",
    "        \n",
    "        return words_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc8d3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Decoder_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention = Masked_Self_Attention()\n",
    "        self.cross_attention = Cross_Attention() # To do\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(EMBED_DIM, 3 * EMBED_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3 * EMBED_DIM, EMBED_DIM)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(EMBED_DIM)\n",
    "        self.norm2 = nn.LayerNorm(EMBED_DIM)\n",
    "        self.norm3 = nn.LayerNorm(EMBED_DIM)\n",
    "        self.dropout1 = nn.Dropout(DROPOUT_RATE)\n",
    "        self.dropout2 = nn.Dropout(DROPOUT_RATE)\n",
    "        self.dropout3 = nn.Dropout(DROPOUT_RATE)\n",
    "    \n",
    "    def forward(self, word_vec, encoder_output):\n",
    "        informed_words = self.attention(word_vec)\n",
    "        informed_words = self.dropout1(informed_words)\n",
    "        word_vec = self.norm1(informed_words + word_vec)\n",
    "\n",
    "        informed_words = self.cross_attention(word_vec, encoder_output)\n",
    "        informed_words = self.dropout2(informed_words)\n",
    "        word_vec = self.norm2(informed_words + word_vec)\n",
    "\n",
    "        informed_words = self.feed_forward(word_vec)\n",
    "        informed_words = self.dropout3(informed_words)\n",
    "        word_vec = self.norm3(informed_words + word_vec)\n",
    "        \n",
    "        return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "804590dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Full_Decoder(nn.Module):\n",
    "    def __init__(self, hindi_input_dim, num_layers): # hindi input vocab size\n",
    "        super().__init__()\n",
    "        self.hindi_embedding = nn.Embedding(hindi_input_dim, EMBED_DIM)\n",
    "        pe = torch.tensor(\n",
    "            create_positional_encoding(MAX_SENT_LEN, EMBED_DIM),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.register_buffer(\"positional_encoding\", pe) # register and save positional encodings\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Transformer_Decoder_Block() for _ in range(num_layers)]\n",
    "        )\n",
    "        self.unembedding = nn.Linear(EMBED_DIM, hindi_input_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, encoder_output):                             # (B, C)\n",
    "        hindi_embedded = self.hindi_embedding(x)               # (B, C, E) batch, context_length, embedding\n",
    "        hindi_words_vec = self.positional_encoding[:hindi_embedded.size(1)] + hindi_embedded   # (B, C, E) Adding positonal embeddings\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            hindi_words_vec = block(hindi_words_vec, encoder_output)\n",
    "        \n",
    "        hindi_words_vec_logits = self.unembedding(hindi_words_vec)\n",
    "        return hindi_words_vec_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e01b6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_Transformer(nn.Module):\n",
    "    def __init__(self, english_vocab, hindi_vocab, num_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = Full_Encoder(english_vocab, num_layers)\n",
    "        self.decoder = Full_Decoder(hindi_vocab, num_layers)\n",
    "    \n",
    "    def forward(self, english_tokens, hindi_tokens):\n",
    "        english_encoder_output = self.encoder(english_tokens)\n",
    "        hindi_vec = self.decoder(hindi_tokens, english_encoder_output)\n",
    "        return hindi_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47ad428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Language_Transformer(eng_tokenizer.vocab_size, hin_tokenizer.vocab_size, NEM_LAYER).to(device)\n",
    "\n",
    "# data_iter = iter(dataset)\n",
    "# sample_batch = next(data_iter)\n",
    "# X_sample = sample_batch['eng_input'].to(device)\n",
    "# y_sample = sample_batch['hin_target'].to(device)\n",
    "# writer.add_graph(model, [X_sample, y_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a35ebbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language_Transformer(\n",
       "  (encoder): Full_Encoder(\n",
       "    (word_embedding): Embedding(45241, 128)\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x Transformer_Encoder_Block(\n",
       "        (self_attention): Self_Attention(\n",
       "          (Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (K): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffnn): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=128, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Full_Decoder(\n",
       "    (hindi_embedding): Embedding(45022, 128)\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x Transformer_Decoder_Block(\n",
       "        (attention): Masked_Self_Attention(\n",
       "          (Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (K): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (cross_attention): Cross_Attention(\n",
       "          (Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (K): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=384, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (unembedding): Linear(in_features=128, out_features=45022, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load saved weights and set model to evaluation mode\n",
    "\n",
    "checkpoint_path = \"../../model/simple_transformer_epoch_10.pth\"\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99830cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=hin_tokenizer.word_index[hin_tokenizer.padding_token])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=START_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59e62a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    # print(f\"Model saved to {filepath}\")\n",
    "    torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1732b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_lr(epoch, total_epochs, start_lr, end_lr):\n",
    "    \"\"\"\n",
    "    Calculates a linearly interpolated learning rate.\n",
    "    \"\"\"\n",
    "    if epoch >= total_epochs:\n",
    "        return end_lr\n",
    "    \n",
    "    # Formula: start + (end - start) * (progress)\n",
    "    lr = start_lr + (end_lr - start_lr) * (epoch / total_epochs)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4859ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 571/571 [02:29<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  5.823859899228853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 571/571 [02:29<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  5.263996583267974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 571/571 [02:30<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  4.883365888807137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 571/571 [02:27<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  4.662611314788115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 571/571 [02:28<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  4.55624800509627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 571/571 [02:29<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  4.492981748844262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 571/571 [02:31<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  4.479096245516415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 571/571 [02:27<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  4.4899276520266564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 571/571 [01:39<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  4.507239602329839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 571/571 [01:36<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:  4.502055239566106\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, TOTAL_EPOCHS):\n",
    "    lr = get_linear_lr(epoch, TOTAL_EPOCHS, START_LR, END_LR)\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    tqdm_bar = tqdm(dataset)\n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    for i, batch in enumerate(tqdm_bar):\n",
    "        # load batch to device\n",
    "        X, y = batch['eng_input'].to(device), batch['hin_target'].to(device)\n",
    "\n",
    "        # print(\"Max Hindi token id:\", y.max().item())\n",
    "        # print(\"Hindi embedding vocab size:\", model.decoder.hindi_embedding.num_embeddings)\n",
    "        # break\n",
    "\n",
    "        decoder_input = y[:, :-1]\n",
    "        decoder_target = y[:, 1:]\n",
    "\n",
    "        # forward pass\n",
    "        logits = model(X, decoder_input)\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            decoder_target.reshape(-1)\n",
    "        )    \n",
    "        writer.add_scalar('Training Loss', loss.item(), epoch * len(tqdm_bar) + i)\n",
    "        writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch * len(tqdm_bar) + i)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tqdm_bar.set_description(f\"Epoch: {epoch+1}\")\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    writer.add_histogram(f'Weights/{name}', param, global_step=i)\n",
    "                    # Logging gradients\n",
    "                    if param.grad is not None:\n",
    "                        writer.add_histogram(f'Gradients/{name}', param.grad, global_step=i)\n",
    "    \n",
    "    k=0\n",
    "    sumer=0\n",
    "    for d in test_dataset:\n",
    "\n",
    "        model.eval()\n",
    "        tX, ty = d['eng_input'].to(device), d['hin_target'].to(device)\n",
    "        decoder_input = ty[:, :-1]\n",
    "        decoder_target = ty[:, 1:]\n",
    "        # test loss\n",
    "        test_logits = model(tX, decoder_input)\n",
    "        test_loss = criterion(test_logits.view(-1, test_logits.size(-1)), decoder_target.reshape(-1))\n",
    "        sumer+=test_loss.item()\n",
    "        k+=1\n",
    "    print(\"Average Test Loss: \",sumer/(k+0.00001))\n",
    "    writer.add_scalar('Test Loss', sumer/(k+0.00001), epoch)\n",
    "    \n",
    "    save_model(model, f\"../../models/simple_transformer_epoch_{epoch+1}.pth\")\n",
    "writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1b3a22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: truck\n",
      "Translation: प्रतिवाद मध्यप्रदेश\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Enter English Sentence To Translate\")\n",
    "\n",
    "def translate(sentence):\n",
    "    model.eval()\n",
    "    # Tokenize English sentence\n",
    "    eng_tokenized = eng_tokenizer.encode([sentence])[0]\n",
    "    if len(eng_tokenized) > MAX_SENT_LEN:\n",
    "        eng_tokenized = eng_tokenized[:MAX_SENT_LEN]\n",
    "    eng_padded = [eng_tokenizer.word_index[eng_tokenizer.padding_token]] * (MAX_SENT_LEN - len(eng_tokenized)) + eng_tokenized\n",
    "    eng_tensor = torch.tensor([eng_padded]).to(device)\n",
    "\n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encoder(eng_tensor)\n",
    "\n",
    "    # Decode (Auto-regressive)\n",
    "    hin_tokenized = [hin_tokenizer.word_index[hin_tokenizer.start_token]]\n",
    "    \n",
    "    for _ in range(MAX_SENT_LEN):\n",
    "        hin_tensor = torch.tensor([hin_tokenized]).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model.decoder(hin_tensor, encoder_output)\n",
    "        \n",
    "        # Get last token logits\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        predicted_token_id = torch.argmax(last_token_logits).item()\n",
    "        \n",
    "        if predicted_token_id == hin_tokenizer.word_index[hin_tokenizer.end_token]:\n",
    "            break\n",
    "            \n",
    "        hin_tokenized.append(predicted_token_id)\n",
    "        \n",
    "    # Decode to text\n",
    "    translated_text = hin_tokenizer.decode([hin_tokenized[1:]])[0] # Skip start token\n",
    "    return translated_text\n",
    "\n",
    "print(f\"Input: {user_query}\")\n",
    "print(f\"Translation: {translate(user_query)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f13ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae31eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
